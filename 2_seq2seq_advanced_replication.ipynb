{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-seq2seq-advanced-replication.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OLjccoUWzjBA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Helpers\n",
        "import numpy as np\n",
        "\n",
        "def batch(inputs, max_sequence_length=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs:\n",
        "            list of sentences (integer lists)\n",
        "        max_sequence_length:\n",
        "            integer specifying how large should `max_time` dimension be.\n",
        "            If None, maximum sequence length would be used\n",
        "    \n",
        "    Outputs:\n",
        "        inputs_time_major:\n",
        "            input sentences transformed into time-major matrix \n",
        "            (shape [max_time, batch_size]) padded with 0s\n",
        "        sequence_lengths:\n",
        "            batch-sized list of integers specifying amount of active \n",
        "            time steps in each input sequence\n",
        "    \"\"\"\n",
        "    \n",
        "    sequence_lengths = [len(seq) for seq in inputs]\n",
        "    batch_size = len(inputs)\n",
        "    \n",
        "    if max_sequence_length is None:\n",
        "        max_sequence_length = max(sequence_lengths)\n",
        "    \n",
        "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
        "    \n",
        "    for i, seq in enumerate(inputs):\n",
        "        for j, element in enumerate(seq):\n",
        "            inputs_batch_major[i, j] = element\n",
        "\n",
        "    # [batch_size, max_time] -> [max_time, batch_size]\n",
        "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
        "\n",
        "    return inputs_time_major, sequence_lengths\n",
        "  \n",
        "  \n",
        "def random_sequences(length_from, length_to, vocab_lower, vocab_upper, batch_size):\n",
        "  \"\"\" Generates batches of random integer sequences,\n",
        "          sequence length in [length_from, length_to],\n",
        "          vocabulary in [vocab_lower, vocab_upper]\n",
        "  \"\"\"\n",
        "  if length_from > length_to:\n",
        "    raise ValueError('length_from > length_to')\n",
        "\n",
        "  def random_length():\n",
        "      if length_from == length_to:\n",
        "          return length_from\n",
        "      return np.random.randint(length_from, length_to + 1)\n",
        "\n",
        "  while True:\n",
        "      yield [\n",
        "          np.random.randint(low=vocab_lower,\n",
        "                            high=vocab_upper,\n",
        "                            size=random_length()).tolist()\n",
        "          for _ in range(batch_size)\n",
        "      ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgGijyJFzj5V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "# sess = tf.InteractiveSession()\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VsvPojug7Tt_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setting up inputs"
      ]
    },
    {
      "metadata": {
        "id": "BRSOgLHs1T1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PAD = 0\n",
        "EOS = 1\n",
        "\n",
        "encoder_hidden_units = 20\n",
        "decoder_hidden_units = encoder_hidden_units * 2 #Why x2? Because decoder state takes in encoder states for forward and backward passes concatenated (each 20)\n",
        "\n",
        "vocab_size = 22 # 20 words + EOS + PAD\n",
        "input_embedding_size = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L6V3gGpR23GQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.placeholder(tf.int32, shape=(None, None), name='encoder_inputs') #shape will be (max_time x batch_size) which will be inferred from the input\n",
        "encoder_inputs_length = tf.placeholder(tf.int32, shape=(None,), name='encoder_inputs_length') #shape will be (batch_size x 1)\n",
        "\n",
        "decoder_targets = tf.placeholder(tf.int32, shape=(None,None), name='decoder_targets') #shape will be (max_time+(some extra space) x batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1yqAFy40321P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = tf.Variable(tf.random_uniform(shape=(vocab_size, input_embedding_size), minval=-1.0, maxval=1.0), dtype=tf.float32) #embedding maps vocab-ranged input to embedding-ranged output\n",
        "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs) # Get embedded inputs that really go into the encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-pXV6uD7S98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setting up the model"
      ]
    },
    {
      "metadata": {
        "id": "NZH_dQby5wnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple #LSTMCell is the class to generate each cell, LSTMStateTuple will be used to combine forward and backward states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zVquHeBBOm_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup Encoder"
      ]
    },
    {
      "metadata": {
        "id": "93txmdRn7q5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_cell = LSTMCell(encoder_hidden_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swoKcVsh88rp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.nn.bidirectional_dynamic_rnn` returns:\n",
        "\n",
        "A tuple (outputs, output_states) where:\n",
        "    outputs: A tuple (output_fw, output_bw) containing the forward and\n",
        "      the backward rnn output `Tensor`.\n",
        "      \n",
        "      If time_major == False (default), output_fw will be a `Tensor` shaped: `[batch_size, max_time, cell_fw.output_size]` and output_bw will be a `Tensor` shaped: `[batch_size, max_time, cell_bw.output_size]`.\n",
        "      \n",
        "      If time_major == True, output_fw will be a `Tensor` shaped: `[max_time, batch_size, cell_fw.output_size]` and output_bw will be a `Tensor` shaped: `[max_time, batch_size, cell_bw.output_size]`.\n",
        "      \n",
        "      It returns a tuple instead of a single concatenated `Tensor`, unlike in the `bidirectional_rnn`. If the concatenated one is preferred, the forward and backward outputs can be concatenated as `tf.concat(outputs, 2)`."
      ]
    },
    {
      "metadata": {
        "id": "Wvg0kTo98Gjr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "((encoder_output_fw,\n",
        "  encoder_output_bw),\n",
        " (encoder_final_state_fw,\n",
        "  encoder_final_state_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
        "                                                      cell_bw=encoder_cell,\n",
        "                                                      inputs=encoder_inputs_embedded,\n",
        "                                                      sequence_length=encoder_inputs_length,\n",
        "                                                      time_major=True,\n",
        "                                                      dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttLqGXt7-Gmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b0682e27-5239-4f64-ed8e-0488fa0108ec"
      },
      "cell_type": "code",
      "source": [
        "encoder_output_fw, encoder_output_bw #shape is (max_time x batch_size x hidden_units)"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>,\n",
              " <tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "metadata": {
        "id": "PEHvktkv-mgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "196e7b2d-2e3a-4056-b493-b64dde298a70"
      },
      "cell_type": "code",
      "source": [
        "encoder_final_state_fw.c, encoder_final_state_fw.h #shape is (batch_size x hidden_units)"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>,\n",
              " <tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_4:0' shape=(?, 20) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "metadata": {
        "id": "SMWUWeNj_Obu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We concatenate forward and backward outputs and forward and backward states"
      ]
    },
    {
      "metadata": {
        "id": "r5-Q2g9g-9Xe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_outputs = tf.concat((encoder_output_fw, encoder_output_bw), axis=2) #concat along axis containing the hidden units' size\n",
        "\n",
        "encoder_final_state_c = tf.concat((encoder_final_state_fw.c, encoder_final_state_bw.c), axis=1) #concat along axis containing the hidden units' size\n",
        "encoder_final_state_h = tf.concat((encoder_final_state_fw.h, encoder_final_state_bw.h), axis=1)\n",
        "\n",
        "encoder_final_state = LSTMStateTuple(c=encoder_final_state_c, h=encoder_final_state_h)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OFNnDTbUFTtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "216cfb08-9723-4421-c813-89941af58667"
      },
      "cell_type": "code",
      "source": [
        "tf.trainable_variables()"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'Variable:0' shape=(22, 20) dtype=float32_ref>,\n",
              " <tf.Variable 'bidirectional_rnn/fw/lstm_cell/kernel:0' shape=(40, 80) dtype=float32_ref>,\n",
              " <tf.Variable 'bidirectional_rnn/fw/lstm_cell/bias:0' shape=(80,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "metadata": {
        "id": "omnPXmUaOisB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set up the decoder"
      ]
    },
    {
      "metadata": {
        "id": "6XpZJ6c3FU9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_cell = LSTMCell(decoder_hidden_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--8ZvWs2PAQC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
        "\n",
        "decoder_lengths = encoder_inputs_length + 3 # +2 for additional steps, +1 for EOS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bj_0WZDlR-LK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eos_time_slice = tf.tile([EOS], [batch_size], name=\"EOS\")\n",
        "pad_time_slice = tf.tile([PAD], [batch_size], name=\"PAD\")\n",
        "\n",
        "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
        "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xiYUN4Dj5cW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random_uniform((decoder_hidden_units, vocab_size), -1, 1), dtype=tf.float32)\n",
        "b = tf.Variable(tf.zeros((vocab_size)), dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2f2ocYLQR-Hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25bcd4e5-287c-445f-9821-ae0fd41bbdba"
      },
      "cell_type": "code",
      "source": [
        "# help(tf.nn.raw_rnn)\n",
        "decoder_cell.trainable_variables"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 315
        }
      ]
    },
    {
      "metadata": {
        "id": "xllkz1piU9zr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_loop_transition():\n",
        "  elements_finished = (0 >= decoder_lengths)\n",
        "  next_input = eos_step_embedded\n",
        "  next_cell_state = encoder_final_state\n",
        "  cell_output = None #There was no output in previous state as this is the first cell\n",
        "  next_loop_state = None #Not passing additional info currently\n",
        "  return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)\n",
        "\n",
        "def loop_transition(time, previous_output, previous_cell_state, previous_loop_state):\n",
        "  def get_next_input():\n",
        "    output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
        "    prediction = tf.argmax(output_logits, axis=1)\n",
        "    next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
        "    return next_input\n",
        "  \n",
        "  elements_finished = (time >= decoder_lengths)\n",
        "  finished = tf.reduce_all(elements_finished)\n",
        "  \n",
        "  next_input = tf.cond(finished, lambda: pad_step_embedded, get_next_input) #If everything is done, then input is PAD, else input is previous cell's output\n",
        "  next_cell_state = previous_cell_state\n",
        "  next_output = previous_output\n",
        "  next_loop_state = None #Not passing additional info\n",
        "  return (elements_finished, next_input, next_cell_state, next_output, next_loop_state)\n",
        "\n",
        "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
        "  if previous_state is None and previous_output is None:\n",
        "    return init_loop_transition()\n",
        "  else:\n",
        "    return loop_transition(time, previous_output, previous_state, previous_loop_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6I3klvIm8qF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf.trainable_variables(), encoder_final_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mtINEGXUIfc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_outputs_ta, final_state, final_loop_state = tf.nn.raw_rnn(cell=decoder_cell, loop_fn=loop_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CGMigxd2hXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_outputs = decoder_outputs_ta.stack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d2DvBxN2-Hvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c7a8b96-f49f-4487-b408-b3cd726898c8"
      },
      "cell_type": "code",
      "source": [
        "decoder_outputs"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 320
        }
      ]
    },
    {
      "metadata": {
        "id": "WjGlD8eYPbqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_max_steps, decoder_batch_size, decoder_units = tf.unstack(tf.shape(decoder_outputs))\n",
        "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_units)) #W is decoder_units x vocab_size\n",
        "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b) #resultant shape = ((decoder_max_steps*batch_size) x vocab_size)\n",
        "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size)) #resultant shape = (decoder_max_steps x batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1K5WktEQwap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_prediction = tf.argmax(decoder_logits, axis=2) #get the chosen word for each time and for each batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XfQLNp48_-yf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "1nnNO-ShAK7d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "crossentropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
        "                                                          logits=decoder_logits) #output shape = max_time x batch_size\n",
        "\n",
        "## One option is to take the mean of crossentropy, which means dividing by max_len*batch_size.\n",
        "## TensorFlow seq2seq tutorial suggests using batch_size only is better\n",
        "loss = tf.reduce_sum(tf.div(crossentropy, tf.cast(batch_size, tf.float32))) \n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Cv2VQfLByUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWIfhnQs_9D3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "91e35aad-4e7a-4137-f45f-e009d95e3c3e"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "batches = random_sequences(length_from=3, length_to=15,\n",
        "                                   vocab_lower=2, vocab_upper=20,\n",
        "                                   batch_size=batch_size)\n",
        "\n",
        "print('head of the batch:')\n",
        "for seq in next(batches)[:10]:\n",
        "    print(seq)"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "head of the batch:\n",
            "[4, 4, 13, 3, 8, 15, 9, 8, 15, 19, 8]\n",
            "[7, 5, 2, 12, 13, 15, 5, 13, 17, 7]\n",
            "[19, 14, 11, 6, 9]\n",
            "[18, 12, 8, 3, 11, 10, 4, 10, 11, 2, 15, 14, 15]\n",
            "[13, 15, 19, 5, 7, 16, 15, 5, 10]\n",
            "[14, 19, 11, 6, 5, 13, 2, 14, 12, 4]\n",
            "[7, 18, 8, 18, 19, 9, 7, 6, 15, 18]\n",
            "[4, 12, 14, 5, 5, 7, 5, 16, 18, 12]\n",
            "[2, 3, 16, 12, 12, 19, 7, 4, 12, 16, 7]\n",
            "[10, 8, 14, 5, 11, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZU-9oLXsBanZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_feed():\n",
        "    batch_ = next(batches)\n",
        "    encoder_inputs_, encoder_input_lengths_ = batch(batch_)\n",
        "    decoder_targets_, _ = batch(\n",
        "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch_]\n",
        "    )\n",
        "    return {\n",
        "        encoder_inputs: encoder_inputs_,\n",
        "        encoder_inputs_length: encoder_input_lengths_,\n",
        "        decoder_targets: decoder_targets_,\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V4zw0qGUBcar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_track = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0dI7z6KbBktE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "373845ee-841a-4bab-9a15-41baeeaa9e12"
      },
      "cell_type": "code",
      "source": [
        "max_batches = 3001\n",
        "batches_in_epoch = 1000\n",
        "\n",
        "try:\n",
        "    for b in range(max_batches):\n",
        "        fd = next_feed()\n",
        "        _, l = sess.run([optimizer, loss], fd)\n",
        "        loss_track.append(l)\n",
        "\n",
        "        if b == 0 or b % batches_in_epoch == 0:\n",
        "            print('batch {}'.format(b))\n",
        "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
        "            predict_ = sess.run(decoder_prediction, fd)\n",
        "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
        "                print('  sample {}:'.format(i + 1))\n",
        "                print('    input     > {}'.format(inp))\n",
        "                print('    predicted > {}'.format(pred))\n",
        "                if i >= 2:\n",
        "                    break\n",
        "            print()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('training interrupted')"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0\n",
            "  minibatch loss: 56.312461853027344\n",
            "  sample 1:\n",
            "    input     > [18  9  2 13 16 14  8 19 12 12  9 10 19  0  0]\n",
            "    predicted > [ 9  9  9  0  0 21 21  5  5  5 18  9  9 20 20 20  0  0]\n",
            "  sample 2:\n",
            "    input     > [14  4  3 15 18  0  0  0  0  0  0  0  0  0  0]\n",
            "    predicted > [18 18  2 18 18  9  9 20  0  0  0  0  0  0  0  0  0  0]\n",
            "  sample 3:\n",
            "    input     > [15 17  5  9  0  0  0  0  0  0  0  0  0  0  0]\n",
            "    predicted > [14 14  9  9 14 14  9  0  0  0  0  0  0  0  0  0  0  0]\n",
            "\n",
            "batch 1000\n",
            "  minibatch loss: 27.00274658203125\n",
            "  sample 1:\n",
            "    input     > [12 18 11 15  2 14 17 13  5 19  0  0  0  0  0]\n",
            "    predicted > [12 12  2 12 19 19 19 19 19 19  1  0  0  0  0  0  0  0]\n",
            "  sample 2:\n",
            "    input     > [ 8  7  4  3 11 13  7  2  2 10  3 16 15 19  5]\n",
            "    predicted > [ 7 13  2  2  2 10 10  5  5  5  5  5 15 19 19  1  0  0]\n",
            "  sample 3:\n",
            "    input     > [14 18 11 11 19 12 11  2 14  6 17 17 13  0  0]\n",
            "    predicted > [11 14 11 11 11 11 11 14 17  6 17 17  1  0  0  0  0  0]\n",
            "\n",
            "batch 2000\n",
            "  minibatch loss: 17.01559829711914\n",
            "  sample 1:\n",
            "    input     > [ 4 14  3 16 19 12 19  0  0  0  0  0  0  0  0]\n",
            "    predicted > [ 4  3 12 19 19 19 19  1  0  0  0  0  0  0  0  0  0  0]\n",
            "  sample 2:\n",
            "    input     > [ 9 11 19  6 12 11 16  4  0  0  0  0  0  0  0]\n",
            "    predicted > [11 11 11 11 11  4  4  4  1  0  0  0  0  0  0  0  0  0]\n",
            "  sample 3:\n",
            "    input     > [11 13 19  6  9  4 18 13 15 14  0  0  0  0  0]\n",
            "    predicted > [11 11 19  6 19  6 18  6  6  6  1  0  0  0  0  0  0  0]\n",
            "\n",
            "batch 3000\n",
            "  minibatch loss: 17.590782165527344\n",
            "  sample 1:\n",
            "    input     > [4 7 8 5 0 0 0 0 0 0 0 0 0 0 0]\n",
            "    predicted > [4 8 5 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  sample 2:\n",
            "    input     > [15  8  6  4 19  5 11  0  0  0  0  0  0  0  0]\n",
            "    predicted > [15  8  6  8  5  5 11  1  0  0  0  0  0  0  0  0  0  0]\n",
            "  sample 3:\n",
            "    input     > [ 7  9  5 16 11 19  5 16 12  7  0  0  0  0  0]\n",
            "    predicted > [ 7 16 16  5  5 19 16 19  7  7  1  0  0  0  0  0  0  0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKiEhJ80BmCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bf505c94-70ae-4110-f5f5-fe77cf81ccdd"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_track)\n",
        "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 17.6785 after 300100 examples (batch_size=100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5ef721dac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "hAr12UtPCf2S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}